# What are Large Language Models?

## Introduction: The Revolution in Natural Language Processing

Large Language Models (LLMs) represent the most significant breakthrough in artificial intelligence since the invention of deep learning itself. These models can write essays, code software, answer complex questions, and engage in nuanced conversations‚Äîcapabilities that seemed science fiction just a decade ago.

But **what exactly are LLMs?** At their core, they are:

> **Neural networks trained on massive text corpora to predict the next token in a sequence, thereby learning statistical patterns of language, knowledge, and reasoning.**

This deceptively simple definition hides profound complexity. Let's unpack it from the ground up.

## The Evolution: From Bag-of-Words to Transformers

### Era 1: Traditional NLP (1950s-2010s)

#### Bag-of-Words and N-grams
Early NLP treated text as collections of words without order. A "bag-of-words" model for "I love machine learning" would be: `{I: 1, love: 1, machine: 1, learning: 1}`.

**Problems**:
- No word order: "Dog bites man" = "Man bites dog"
- No context: "bank" (river) vs "bank" (financial institution)
- Sparse representations: Vocabulary size = vector dimension

#### TF-IDF and Feature Engineering
Term Frequency-Inverse Document Frequency (TF-IDF) weighted words by importance:
```
TF-IDF(word, doc) = (word count in doc) √ó log(total docs / docs containing word)
```

This helped but still required manual feature engineering for every task.

### Era 2: Neural Language Models (2010-2017)

#### Word Embeddings (Word2Vec, GloVe)
In 2013, Mikolov et al. introduced Word2Vec‚Äîrepresenting words as dense vectors in ~300 dimensions. The magic: similar words cluster together geometrically.

```python
# Conceptual example
king - man + woman ‚âà queen
```

**Breakthrough**: Semantic relationships encoded in vector arithmetic!

#### Recurrent Neural Networks (RNNs) and LSTMs
RNNs processed sequences step-by-step:

```
h_t = tanh(W_h * h_{t-1} + W_x * x_t + b)
```

Where:
- `h_t` = hidden state at time t
- `x_t` = input at time t
- `W_h`, `W_x` = learned weight matrices

**LSTMs (Long Short-Term Memory)** added gates to remember long-range dependencies:

```
forget_gate = œÉ(W_f * [h_{t-1}, x_t] + b_f)
input_gate = œÉ(W_i * [h_{t-1}, x_t] + b_i)
output_gate = œÉ(W_o * [h_{t-1}, x_t] + b_o)
```

**Problems**:
- Sequential processing (no parallelization)
- Vanishing gradients for long sequences
- Limited context window (~100-200 tokens)

### Era 3: The Transformer Revolution (2017-Present)

#### "Attention Is All You Need" (2017)
The Vaswani et al. paper introduced transformers with **self-attention**‚Äîthe killer feature that changed everything.

**Key innovation**: Instead of processing tokens sequentially, attention computes relationships between all tokens in parallel:

```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
```

Where:
- Q (Query): "What am I looking for?"
- K (Key): "What do I offer?"
- V (Value): "What information do I carry?"

**Analogy**: Imagine a library where every book (token) can instantly check its relevance to every other book. Instead of reading sequentially, you spotlight relevant information in parallel.

#### Why Transformers Won

1. **Parallelization**: Process all tokens simultaneously (vs. sequential RNNs)
2. **Long-range dependencies**: Direct connections between distant tokens
3. **Scalability**: Performance improves predictably with more data and compute
4. **Transfer learning**: Pre-train once, fine-tune for many tasks

## The Modern LLM Era: 2018-2025

### Phase 1: Foundational Models (2018-2020)

#### BERT (Google, 2018)
- **Architecture**: Bidirectional encoder (reads text both directions)
- **Training**: Masked Language Modeling (predict hidden words)
- **Size**: 110M-340M parameters
- **Impact**: Dominated NLU tasks (question answering, classification)

```python
# BERT's training objective (simplified)
input: "The [MASK] sat on the mat"
target: Predict "cat"
```

#### GPT-2 (OpenAI, 2019)
- **Architecture**: Unidirectional decoder (predicts next word)
- **Training**: Auto-regressive language modeling
- **Size**: 1.5B parameters
- **Impact**: Showed emergent generation abilities

```python
# GPT-2's training objective
input: "The cat sat on"
target: Predict "the"
```

### Phase 2: Scale and Emergence (2020-2022)

#### GPT-3 (OpenAI, 2020)
- **Size**: 175B parameters (100x larger than GPT-2!)
- **Training data**: ~500B tokens from Common Crawl, books, Wikipedia
- **Cost**: ~$5M in compute
- **Breakthrough**: Few-shot learning‚Äîperform tasks with just examples

```python
# Few-shot prompting
prompt = """
Translate English to French:
English: Hello
French: Bonjour

English: Goodbye
French: Au revoir

English: Thank you
French:
"""
# GPT-3 outputs: "Merci"
```

**Emergent abilities**: As models scaled, new capabilities appeared that weren't explicitly trained:
- Arithmetic
- Code generation
- Multi-step reasoning
- Instruction following

#### Scaling Laws (Kaplan et al., 2020)
Discovered predictable relationships:
```
Loss ‚àù N^(-Œ±)  # N = number of parameters
Loss ‚àù D^(-Œ≤)  # D = dataset size
Loss ‚àù C^(-Œ≥)  # C = compute budget
```

**Implication**: Bigger models + more data + more compute = better performance (until data quality plateaus)

### Phase 3: Alignment and Chat (2022-2023)

#### ChatGPT (OpenAI, November 2022)
Built on GPT-3.5 with **RLHF (Reinforcement Learning from Human Feedback)**:

1. **Supervised fine-tuning**: Train on high-quality conversations
2. **Reward modeling**: Humans rank outputs (helpful, harmless, honest)
3. **PPO optimization**: Reinforce behaviors with high rewards

```python
# RLHF pipeline (simplified)
# Step 1: Supervised fine-tuning
model.train(instruction_following_examples)

# Step 2: Train reward model
reward_model.train(human_preferences)

# Step 3: PPO (Proximal Policy Optimization)
for prompt in dataset:
    responses = model.generate(prompt, n=4)
    scores = reward_model(responses)
    model.update_policy(maximize=scores)
```

**Impact**: 100M users in 2 months‚Äîfastest product adoption in history.

#### GPT-4 (OpenAI, March 2023)
- **Multimodal**: Processes images and text
- **Size**: Rumored ~1.7T parameters (Mixture-of-Experts)
- **Performance**: 90th percentile on bar exam, 5 on AP exams
- **Capabilities**: Complex reasoning, creative writing, code debugging

### Phase 4: Open-Source and Efficiency (2023-2025)

#### Llama (Meta, 2023-2024)
- **Philosophy**: Open weights for research
- **Sizes**: 7B, 13B, 70B parameters
- **Training**: 2T tokens on public data
- **Impact**: Enabled community fine-tuning, LoRA, quantization

```python
# Fine-tune Llama with LoRA (Low-Rank Adaptation)
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,  # Rank of low-rank matrices
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(base_model, config)
# Now train only ~1% of parameters!
```

#### Claude (Anthropic, 2023-2024)
- **Constitutional AI**: Model follows a "constitution" of principles
- **Long context**: 100K-200K token windows
- **Safety**: Built-in harmlessness via RLHF

#### Mixture-of-Experts (MoE)
Models like **Mixtral** and **Grok-2** use sparse activation:
- Only activate relevant "expert" sub-networks per token
- 8x56B = 448B total parameters, but only ~56B active
- **Efficiency**: Performance of dense 140B with cost of dense 14B

```python
# MoE routing (simplified)
def forward(x, experts, router):
    # Router selects top-k experts per token
    expert_indices = router(x).topk(k=2)
    
    # Only compute selected experts
    output = sum(expert[i](x) for i in expert_indices)
    return output
```

#### OpenAI o1 (2024)
- **Reasoning focus**: Uses "chain-of-thought" internally before answering
- **Performance**: PhD-level problems in physics, math, biology
- **Architecture**: Extended "thinking time" during inference

## What Makes LLMs "Large"?

### Scale Dimensions

1. **Parameters**: Trainable weights (GPT-3: 175B, GPT-4: ~1.7T)
2. **Training data**: Tokens seen (GPT-3: 300B, Llama-2: 2T)
3. **Compute**: FLOPs required (GPT-3: ~3√ó10¬≤¬≥)
4. **Context window**: Max input length (GPT-4: 32K-128K tokens)

### Emergent Abilities

Small models (~1B params) struggle with:
- Multi-step reasoning
- Following complex instructions
- In-context learning (few-shot)

Large models (100B+ params) gain:
- **Zero-shot task transfer**: Perform new tasks without examples
- **Chain-of-thought**: Break problems into steps
- **Tool use**: Call APIs, run code, search databases
- **Multi-modal understanding**: Process images, audio, video

**Example**:
```
Prompt: "A farmer has 17 sheep. All but 9 die. How many are left?"

Small model: "8" (arithmetic: 17-9)
Large model: "9" (understands 'all but 9' means 9 survive)
```

## How LLMs Learn

### Pre-training: The Foundation

**Objective**: Predict next token given context
```
Input:  "The cat sat on the"
Target: "mat"
```

**Dataset**: Trillions of tokens from:
- Common Crawl (web pages)
- Books (Gutenberg, books3)
- Wikipedia
- GitHub (code)
- News articles

**Training dynamics**:
```python
# Simplified training loop
for epoch in epochs:
    for batch in dataloader:
        # Forward pass
        logits = model(batch.input_ids)
        loss = cross_entropy(logits, batch.labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

**Compute requirements** (GPT-3 scale):
- **Hardware**: 10,000 V100 GPUs
- **Time**: ~1 month
- **Cost**: ~$5M
- **Energy**: ~1.3 GWh (enough to power 100 homes for a year)

### Fine-tuning: Task Specialization

After pre-training, adapt to specific tasks:

**Supervised fine-tuning**:
```python
# Example: Code generation
examples = [
    {"prompt": "Write a Python function to reverse a string",
     "completion": "def reverse_string(s): return s[::-1]"},
    # ... thousands more
]
model.finetune(examples, epochs=3)
```

**LoRA (Low-Rank Adaptation)**:
Instead of updating all 175B parameters, add small adapter matrices:
```
W_new = W_frozen + A @ B  # A, B are small (rank r << d)
```

Only train A and B (~0.1% of parameters), keeping base model frozen.

### RLHF: Alignment with Human Values

Three-step process:

1. **SFT (Supervised Fine-Tuning)**: Train on high-quality human demonstrations
2. **Reward Modeling**: Train a model to score outputs based on human preferences
3. **PPO (Proximal Policy Optimization)**: Optimize policy to maximize reward

```python
# RLHF reward function
def reward(prompt, response):
    score = reward_model(prompt, response)
    
    # Penalize divergence from base model
    kl_penalty = KL(response_policy || base_policy)
    
    return score - Œ≤ * kl_penalty
```

**Result**: Models that are helpful, harmless, and honest.

## The Transformer Architecture (Preview)

We'll dive deep in Module 02, but here's the high-level structure:

```
Input text ‚Üí Tokenization ‚Üí Embeddings ‚Üí Positional Encoding
    ‚Üì
[Transformer Block] √óN
    ‚îú‚îÄ Multi-Head Attention
    ‚îú‚îÄ Layer Normalization
    ‚îú‚îÄ Feed-Forward Network
    ‚îî‚îÄ Residual Connections
    ‚Üì
Output Logits ‚Üí Softmax ‚Üí Predicted Token
```

**Key components**:
- **Attention**: Relates tokens to each other
- **Feed-forward**: Processes each token independently
- **Layer norm**: Stabilizes training
- **Residuals**: Enables deep networks (GPT-3 has 96 layers!)

## Capabilities and Limitations

### What LLMs Can Do (2025)

‚úÖ **Text generation**: Essays, stories, poetry
‚úÖ **Code**: Write, debug, explain programs in 50+ languages
‚úÖ **Reasoning**: Multi-step math, logic puzzles
‚úÖ **Translation**: 100+ language pairs
‚úÖ **Summarization**: Condense articles, papers, meetings
‚úÖ **Question answering**: Factual and open-ended
‚úÖ **Instruction following**: Complex, multi-step tasks
‚úÖ **Few-shot learning**: New tasks from examples
‚úÖ **Tool use**: Call APIs, search, execute code

### What LLMs Can't Do (Yet)

‚ùå **True understanding**: No grounding in physical reality
‚ùå **Perfect factuality**: Hallucinate confidently
‚ùå **Long-term memory**: Context window limits (even at 200K tokens)
‚ùå **Real-time learning**: Can't update knowledge without retraining
‚ùå **Consistent reasoning**: Brittle on adversarial examples
‚ùå **Multimodal generation**: Text-to-video still nascent

**Example hallucination**:
```
Prompt: "Who won the Nobel Prize in Physics in 2025?"
LLM: "Dr. Jane Smith won for her work on quantum gravity"
# (Plausible but completely fabricated)
```

## Real-World Applications

### 1. Code Assistants
- **GitHub Copilot**: Auto-complete code from comments
- **AlphaCode**: Competitive programming (Codeforces ~50th percentile)
- **Codex**: Powers OpenAI API code capabilities

### 2. Writing and Content
- **Jasper.ai**: Marketing copy, blog posts
- **Copy.ai**: Ad copy, social media
- **Writesonic**: Long-form content

### 3. Customer Support
- **Intercom**: Automated responses with escalation
- **Ada**: Conversational AI for support
- **Zendesk**: Ticket routing and suggested responses

### 4. Education
- **Khan Academy's Khanmigo**: Personalized tutoring
- **Duolingo Max**: Language learning with GPT-4
- **This repository!**: AI Tutor for any subject

### 5. Research and Analysis
- **Elicit**: Literature review and synthesis
- **Consensus**: Evidence-based answers from papers
- **Perplexity**: Search with citations

## The Economics of LLMs

### Training Costs

| Model | Parameters | Compute (FLOPs) | Cost (2023 prices) |
|-------|-----------|-----------------|-------------------|
| BERT-Large | 340M | ~10¬π‚Å∏ | $1K-$5K |
| GPT-2 | 1.5B | ~10¬π‚Åπ | $50K |
| GPT-3 | 175B | ~10¬≤¬≥ | $5M |
| GPT-4 | ~1.7T | ~10¬≤‚Å¥ | $50M-$100M |
| Frontier (2025) | 10T+ | ~10¬≤‚Åµ+ | $500M-$1B |

### Inference Costs

**GPT-3** (175B parameters):
- Per token: ~$0.0001 (with caching and optimization)
- Per 1M tokens: ~$100
- ChatGPT query (200 tokens): ~$0.02

**Cost optimization techniques**:
- Quantization: 8-bit (2x speedup), 4-bit (4x speedup)
- Distillation: Smaller student model (10x cheaper)
- Speculative decoding: Draft with small model, verify with large
- Batching: Serve multiple requests together

### Business Models

1. **API-as-a-Service**: OpenAI, Anthropic, Cohere ($0.001-$0.06 per 1K tokens)
2. **Fine-tuning marketplace**: Scale AI, Snorkel
3. **Managed deployment**: Hugging Face Inference Endpoints
4. **Open-source + consulting**: Databricks, Together AI
5. **Vertical solutions**: Jasper (marketing), Codeium (coding)

## Ethical Considerations

### Bias and Fairness
LLMs learn from internet data, including:
- Gender stereotypes ("nurse" ‚Üí female, "doctor" ‚Üí male)
- Racial bias in sentiment analysis
- Political lean based on training data sources

**Mitigation**:
- Curate training data (remove toxic content)
- RLHF with diverse annotators
- Red-teaming for adversarial prompts

### Misinformation
LLMs can generate convincing but false text:
- Fake news articles
- Academic paper fabrication
- Impersonation

**Mitigation**:
- Watermarking generated text
- Citation and source linking
- Uncertainty calibration ("I'm not sure...")

### Privacy
Training on public data may include:
- Personal information
- Copyrighted material
- Confidential leaks

**Mitigation**:
- Data filtering and PII removal
- Opt-out mechanisms
- Differential privacy in training

### Environmental Impact
Training GPT-3 emitted ~552 tons CO‚ÇÇ (equivalent to 5 round-trip flights NYC to SF per person).

**Mitigation**:
- Carbon-aware computing (train during low-carbon hours)
- Efficient architectures (MoE, sparse models)
- Reuse pre-trained models (fine-tuning uses 0.01% of training compute)

## The Future: Where Are We Heading?

### Next 2-3 Years (2025-2027)

1. **Multimodal mastery**: Seamless text, image, video, audio
2. **Longer context**: 1M+ token windows (entire books)
3. **Better reasoning**: o1-style thinking for all models
4. **Personalization**: Models that learn your preferences in-context
5. **Tool integration**: LLMs orchestrating software, APIs, robotics

### Next 5-10 Years (2027-2035)

1. **AGI (Artificial General Intelligence)**: Human-level on all cognitive tasks?
2. **Continual learning**: Update knowledge without full retraining
3. **Embodied AI**: LLMs controlling robots in physical world
4. **Scientific discovery**: Novel hypotheses, theorem proving
5. **Democratization**: 7B models outperform today's GPT-4

### Open Questions

- Will scaling continue to work? (Or hit data/compute walls?)
- Can we achieve interpretability? (Understand what models "think")
- How to align superhuman AI? (Control systems smarter than us)
- What happens to jobs? (Creative professions, coding, writing)

## Summary: The LLM Landscape (2025)

**Closed-Source Leaders**:
- **OpenAI**: GPT-4, o1 (reasoning), ChatGPT
- **Anthropic**: Claude (safety, long context)
- **Google**: Gemini (multimodal, integrated with search)

**Open-Source Champions**:
- **Meta**: Llama (7B-70B, community favorite)
- **Mistral AI**: Mixtral (MoE, efficient)
- **Microsoft/Alibaba**: Phi, Qwen (small but mighty)

**Specialized Models**:
- **Code**: CodeLlama, StarCoder, WizardCoder
- **Math**: Llemma, MAmmoTH
- **Embedding**: BGE, E5, Instructor

## Conclusion

Large Language Models are not just scaled-up versions of previous NLP systems‚Äîthey represent a qualitative shift in what's possible with AI. By training on trillions of tokens and scaling to hundreds of billions of parameters, these models develop emergent capabilities that enable them to:

- Generate human-quality text across domains
- Reason through complex problems step-by-step
- Adapt to new tasks with minimal examples
- Serve as building blocks for agentic AI systems

The journey from bag-of-words to GPT-4 took 70 years. The next 5 years promise even more dramatic leaps.

**In the next section**, we'll get hands-on with tokenization‚Äîthe critical first step in every LLM pipeline.

---

## Visualization: LLM Evolution Timeline

```
1950s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Symbolic AI, rule-based systems                          ‚îÇ
‚îÇ                                                           ‚îÇ
2000s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Bag-of-Words, TF-IDF                                     ‚îÇ
‚îÇ                                                           ‚îÇ
2013 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Word2Vec (Mikolov et al.) - Dense embeddings            ‚îÇ
‚îÇ                                                           ‚îÇ
2014 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Seq2Seq with Attention (Bahdanau et al.)                ‚îÇ
‚îÇ GRU, LSTM - Recurrent architectures                     ‚îÇ
‚îÇ                                                           ‚îÇ
2017 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üî• TRANSFORMERS (Vaswani et al.)                        ‚îÇ
‚îÇ    "Attention Is All You Need"                           ‚îÇ
‚îÇ                                                           ‚îÇ
2018 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ GPT (117M params) - Generative pre-training             ‚îÇ
‚îÇ BERT (340M) - Bidirectional encoder                     ‚îÇ
‚îÇ                                                           ‚îÇ
2019 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ GPT-2 (1.5B) - "Too dangerous to release"              ‚îÇ
‚îÇ RoBERTa, DistilBERT - BERT improvements                 ‚îÇ
‚îÇ                                                           ‚îÇ
2020 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üöÄ GPT-3 (175B) - Few-shot learning emergence          ‚îÇ
‚îÇ T5, BART - Encoder-decoder models                       ‚îÇ
‚îÇ                                                           ‚îÇ
2021 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Codex - Code generation                                 ‚îÇ
‚îÇ DALLE - Text-to-image                                   ‚îÇ
‚îÇ Gopher, Chinchilla - Scaling law refinements            ‚îÇ
‚îÇ                                                           ‚îÇ
2022 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üî• ChatGPT (GPT-3.5 + RLHF) - 100M users               ‚îÇ
‚îÇ InstructGPT - Instruction following                     ‚îÇ
‚îÇ                                                           ‚îÇ
2023 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üåü GPT-4 (multimodal, 1.7T MoE)                        ‚îÇ
‚îÇ Claude (Constitutional AI, 100K context)                ‚îÇ
‚îÇ Llama (open weights, 7B-70B)                            ‚îÇ
‚îÇ Mixtral (MoE, efficient)                                ‚îÇ
‚îÇ                                                           ‚îÇ
2024 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ OpenAI o1 (reasoning focus)                             ‚îÇ
‚îÇ Llama 3 (improved, 400B)                                ‚îÇ
‚îÇ Gemini 1.5 (1M+ context window)                         ‚îÇ
‚îÇ                                                           ‚îÇ
2025+ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üîÆ Next-gen models (multimodal, agentic, reasoning)    ‚îÇ
‚îÇ 10T+ parameters, AGI research                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key milestones highlighted**:
- 2017: Transformer architecture (paradigm shift)
- 2020: GPT-3 (emergent abilities at scale)
- 2022: ChatGPT (mainstream adoption)
- 2023-2025: Open-source explosion + reasoning breakthroughs

---

**Next**: Deep dive into tokenization ‚Üí `02_tokenization.md`
